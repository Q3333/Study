# 200402_W3D2_Cross Entropy, Logistic





# 머신러닝(전 주에 한거)





## Entropy(불확실성?)

![1585550253827](assets/1585550253827.png)



비가오거나 해가 뜨는 2가지 밖에 없을 경우 1비트로 정보를 보낼 수 있다.

![1585550357430](assets/1585550357430.png)



0or 1로



만약 날씨의 경우의 수가 8가지라면? 

-> 3비트로 충분하다.

![1585550413499](assets/1585550413499.png)





결론 : ![1585550436279](assets/1585550436279.png)





만약 언제나 맑음일때는 ?

![1585550512551](assets/1585550512551.png)

0비트. 즉 알려줄 필요가 없다.





즉 정보의 확률에 따라 필요 bit 가 달라진다.



또한 정보가 불확실할때 필요한 bit양이 커진다.







![1585550669717](assets/1585550669717.png)



비트수 x 확률 까지 해주어야함.









![1585550709158](assets/1585550709158.png)



가장 불 확실할 때가 3비트.



가장 많이 나올 확률인 2개를 00,01 두비트만 사용하고



거의 나오지 않을 확률을 5비트를 주는 식으로 할당하면? 2.42의 결과값이나옴.



실제 확률을 엔트로피로 계산하면 2.23이  나왔는데

아래에  추정해서 계산한 값은 2.42값이 나옴.





이때, 실제확률과 예측한 확률의 차이를 

cross-entropy 라고함.





![1585550951747](assets/1585550951747.png)



예측확률과 실제 확률이 같아지면? -> Cross-entropy값이 entropy 값이 된다.



만약 예측 entropy가 더 크면 값의 차이가 있다는 뜻. (클수록 차이도 큼)







![1585551091993](assets/1585551091993.png)

KL-divergence를 줄인다는 것은 실제 확률값과 예측 확률값을 줄인다. 라는 의미다.







---------------------------------------------

# 0402 ~

크로스 엔트로피는 확률 분포의 거리이다.



확률 분포의 거리를 나타내는 방법은 다른 방법도 존재.



#### 왜 크로스 엔트로피를 쓰느냐?

-> 특별한 특성이 있기 때문 (Convex, 장애물이없음)



현재 모델

![1585800395321](assets/1585800395321.png)



w와b를 최소화 하는 변수로 튜닝해야함.





Binary Classfication 의 식

![1585800434387](assets/1585800434387.png)



기울기가 >= 0 

즉,



![1585800552293](assets/1585800552293.png)



이러한 형태를 가져야함.



L이 w방향으로 Convex함.



Minimize하는 w값을 찾는 것이 목표.



![1585800625840](assets/1585800625840.png)





w를 찾는 방법은 ? 장애물이 없다는 가정(Convex)에 아래로 쭉 내려가다보면 낮은 지점에 도착함.



The cross-entropy function of logistic regression 에서는 항상 Convex(장애물X)하다.



#### 그러므로 크로스 엔트로피를 쓰는 이유는 Convex하기 때문이다.





## Gradient Descent (경사 하락)



우리의 목표는 ?



![1585801770039](assets/1585801770039.png)



최소화된 지점을 가야한다. (빨간 화살표)



![1585801816771](assets/1585801816771.png)



이러한 곡선이 있을 때 오른쪽에서 시작을 하면 Convex하기 때문에 아무 문제 없이 최저값을 찾을 수 있음.



##### 그럼 왼쪽에서는? 

수학적인 완벽한 방법이 없음.

![1585801882461](assets/1585801882461.png)

##### 왔다갔다하면서 자신이 현재 local 최저점인지 , global 최저점인지를 파악하는 것이 중요.



#### 알고리즘

![1585801941957](assets/1585801941957.png)



![1585801987382](assets/1585801987382.png)



w값을 튜닝하기 위해서는 기울기가 낮아지는 값(dw/dl)을 구한뒤 알파(a)를 곱한 값을 빼 주어야 한다.





#### w와 b의 확장

![1585802079129](assets/1585802079129.png)





### Computation graph



![1585802165623](assets/1585802165623.png)



펄셉트론 이론은 모양을 본뜻 것이지, 신경세포와 같지 않다. (기능적으로 다름)



그럼 뉴럴 네트워크는? 

Computation graph임.



![1585802222086](assets/1585802222086.png)

이러한 그림을 전전파(Forward propagation) 라고 부름.

반대방향은 역전파(Backward propagation) <-





### Computation graph with back propagation



![1585802285206](assets/1585802285206.png)

##### 역전파 = chain rule임



간단한 미분은 직접 가능하지만 길어지거나 복잡해질 경우?

back propagation을 한다.







# 로지스틱 최종



앞서 배운 내용들을 토대로 로지스틱 분석을 짬.



![1585802511023](assets/1585802511023.png)



chain rule로 구함.



![1585802596176](assets/1585802596176.png)



L=크로스엔트로피

a=y^(y햇)



![1585802647307](assets/1585802647307.png)



각각 미분으로 구함



시그모이드 함수

![1585802698041](assets/1585802698041.png)



시그모이드를 미분한 값

![1585802680543](assets/1585802680543.png)





두번쨰 텀



![1585802739209](assets/1585802739209.png)



w1으로 미분하면 x1이 나옴



전체 그림

![1585802788306](assets/1585802788306.png)







![1585802849502](assets/1585802849502.png)







### 코딩을 짜기 위한 알고리즘

![1585802905771](assets/1585802905771.png)



위의 3식이 그동안 유도했던 3개의 식



아래가 코딩



![1585802949805](assets/1585802949805.png)



순서대로 z는 Linear

a는 Non-Linear

J 는 += 이므로 누적합 ( 끝에 m으로 나눠줌 )

![1585802997369](assets/1585802997369.png)

미분(누적합)





![1585803013980](assets/1585803013980.png)

전체 식









![1585803134762](assets/1585803134762.png)

![1585803199074](assets/1585803199074.png)



한스텝을 밟기 위해 n번의 for문을 돌고 

![1585803219313](assets/1585803219313.png)



계속해서 이동을 해야하기 때문에 이중for문으로 돌려야 한다.





다음 시간에는 이걸 조금 더 빠르게 만드는 방법을 알아볼 예정.