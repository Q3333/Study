{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "pytorch_lenet_.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tCipQ-4nGpr",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Networks: MNIST\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "- Implement helper functions that you will use when implementing a PyTorch model\n",
        "- Implement a fully functioning ConvNet using PyTorch \n",
        "\n",
        "**After this assignment you will be able to:**\n",
        "\n",
        "- Build and train a ConvNet in PyTorch for a classification problem \n",
        "\n",
        "We assume here that you are already familiar with PyTorch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aatpvdbgms50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "5573fbef-e6c3-4461-8f9b-852bd9ae296a"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('using device:', device)\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**2,1), 'MB')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device: cuda\n",
            "Tesla P4\n",
            "Memory usage:\n",
            "Allocated: 172.7 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BKpoUNNnQxA",
        "colab_type": "text"
      },
      "source": [
        "## 1. PyTorch model\n",
        "\n",
        "### 1.1. Load dataset (MNIST) \n",
        "\n",
        "Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. \n",
        "\n",
        "We will start by loading in the packages. \n",
        "\n",
        "The MNIST Dataset is located at http://yann.lecun.com/exdb/mnist/. Each image has $28 \\times 28$ dimension.\n",
        "\n",
        "-train-images-idx3-ubyte.gz:  training set images (9912422 bytes) including 55000 examples <br>\n",
        "-train-labels-idx1-ubyte.gz:  training set labels (28881 bytes) <br>\n",
        "-t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes) including 10000 examples <br>\n",
        "-t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFF0e5udms54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = datasets.MNIST( root='./mnist_data/',\n",
        "                              train=True,\n",
        "                              transform=transforms.ToTensor(),\n",
        "                              download=True)\n",
        "test_dataset = datasets.MNIST( root='./mnist_data/',\n",
        "                             train = False,\n",
        "                             transform=transforms.ToTensor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwkQNi5foOTO",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Prepare dataset\n",
        "\n",
        "Let's prepare the dataset using **torch.tuils.data.Dataloader**\n",
        "\n",
        "PyTorch provides DataLoader for the input data (training/testing) that will be fed into the model when training/testing the model.\n",
        "\n",
        "**Exercise**: Implement the function below to prepare the training/testing data. You can define the number of examples for the moment and shuffle the order of examples by setting batch_size and shuffle.\n",
        "\n",
        "**The constructor arguments of a DataLoader :**\n",
        "```python\n",
        "torch.utils.data.DataLoader (dataset, batch_size=1, shuffle=False, sampler=None,\n",
        "                            batch_sampler=None, num_workers=0, collate_fn=None,\n",
        "                            pin_memory=False, drop_last=False, timeout=0,\n",
        "                            worker_init_fn=None)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDNXScGRoIEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Set the batch_size \n",
        "batch_size = 64\n",
        "\n",
        "### START CODE HERE ### (â‰ˆ1 lines)\n",
        "#Use torch.utils.data.DataLoader() with bath_size 64, shuffle = True \n",
        "train_loader = None\n",
        "### END CODE HERE ###  \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader( dataset=test_dataset,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "# train_loader has (x, y) where x.size()=[64, 1, 28, 28] and y.size()=[64]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpdTftvIpRFs",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Forward propagation\n",
        "\n",
        "In PyTorch, there are built-in functions that carry out the convolution steps for you.\n",
        "\n",
        "- **torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):** Applies a 2D convolution over an input signal composed of several input planes.  \n",
        "\n",
        "- **torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False):** applies a 2D max pooling over an input signal composed of several input planes.\n",
        "\n",
        "- **torch.nn.Linear(in_features, out_features, bias=True):** Applies a linear transformation to the incoming data\n",
        "\n",
        "- **torch.nn.functional.relu(input, inplace=False):** Applies the rectified linear unit function element-wise \n",
        "\n",
        "- **torch.nn.functional.softmax(dim=None):** torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
        "\n",
        "- **torch.flatten(input, start_dim=0, end_dim=-1) :** Flattens a contiguous range of dims in a tensor.\n",
        "\n",
        "**For more details:**\n",
        "\n",
        "torch.nn : https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "torch.functional : https://pytorch.org/docs/stable/nn.functional.html\n",
        "\n",
        "torch.flatten : https://pytorch.org/docs/master/generated/torch.flatten.html\n",
        "\n",
        "\n",
        "**Exercise**: \n",
        "\n",
        "Implement the `NNModel` class below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`. You should use the functions above. \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bDY8CEgms6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Implements forward propabation for the CNN model:\n",
        "CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
        "\n",
        "\"\"\"\n",
        "class NNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5, padding=(2,2)) # input channel, output channels, and filter size\n",
        "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        # Define the Convolution layer(Condv2d) with (32:input channel, 64:output channels, 5: filter size) and the 2 padding. \n",
        "        self.conv2 = None  \n",
        "        # Define the Maxpooling layer(MaxPool2d) with (2:filter size, 2:stride)\n",
        "        self.pool2 = None\n",
        "        # Define the fully-conneted layer(Linear) with (3136:input channel, 1024:output channel)\n",
        "        self.fc1 = None\n",
        "        ### END CODE HERE ###\n",
        "        self.fc2 = nn.Linear(1024,10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            Arguments:\n",
        "            X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "            parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
        "                          the shapes are given in initialize_parameters\n",
        "\n",
        "            Returns:\n",
        "            F.softmax(x) -- the softmax output of the last LINEAR unit\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        ### START CODE HERE ###\n",
        "        # Load sceond Convoultion layer(conv2d) \n",
        "        x = None\n",
        "        # Load Relu function  \n",
        "        x = None\n",
        "        # Load sceond Maxpooling layer(MaxPool2d) \n",
        "        x = None\n",
        "        # Flatten each example into a 1D vector: (?,7,7,64)->(?, 3136)\n",
        "        x = None\n",
        "        # Load first fully-connted layer \n",
        "        x = None \n",
        "        ### END CODE HERE ### \n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x)    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvmWxnal0hWA",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Build model\n",
        "\n",
        "Finally you will merge the helper functions you implemented above to build and train a model.  \n",
        "\n",
        "**Exercise**: Complete the function below. \n",
        "\n",
        "The model below should:\n",
        "\n",
        "- Define the cost function with **torch.nn.CrossEntropyLoss()**    \n",
        "> torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') \n",
        "> **For more details:** https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "- Create optmizization function with **torch.optim.SGD()** \n",
        "> torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)     \n",
        "> **For more details:** https://pytorch.org/docs/stable/optim.html\n",
        "- hint : For getting params for optmizer, use the following code\n",
        "```python\n",
        "params = model.parameters()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "akEyMJMxms6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = NNModel()\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "model.apply(weights_init)\n",
        "\n",
        "### START CODE HERE ###  \n",
        "# Define the cost function with cross entropy      \n",
        "criterion = None  \n",
        "# Create optimizer \n",
        "# Get model paramters \n",
        "params = None\n",
        "# Define the optimizer (using SGD) with learing rate=0.01 and momentum=0.5\n",
        "optimizer = None\n",
        "### END CODE HERE ###\n",
        "\n",
        "model = model.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "701_v22n3KxE",
        "colab_type": "text"
      },
      "source": [
        "## 2. Train and test model\n",
        "\n",
        "Finally you will create training and testing function with a loop for each mini-batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-cfYlJx4ig1",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Train function\n",
        "\n",
        "Implement the function to train the CNN model.\n",
        "\n",
        "**Exercise**: Complete the function below. \n",
        "\n",
        "- copy a tensor on the CPU to the GPU \n",
        "- call the CNN model \n",
        "- initialize the optimizer  \n",
        "- compute the cost  \n",
        "- compute the backward propagation\n",
        "- update parameters with the optimizer \n",
        "\n",
        "**Hint**\n",
        "- copy a tensor on the CPU to the GPU\n",
        "         # copy a tensor to the CPU \n",
        "         data = data.to(\"cpu\")\n",
        "         # copy a tensor to the GPU \n",
        "         data = data.to(\"cuda\") \n",
        "- call the CNN model : call the predefined CNN model funciton    \n",
        "- initialize optimizer \n",
        "        optimizer.zero_grad() \n",
        "- compute the cost : call the predifined criterion function  \n",
        "- compute the backward propagation   \n",
        "        loss.backward()\n",
        "- update parameters with the optimizer \n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40y8dtLi24_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        \n",
        "        ### START CODE HERE ###  \n",
        "        # Copy \"data\" tensor to the GPU\n",
        "        data = None\n",
        "        # Copy \"target\" tensor to the GPU\n",
        "        target = None\n",
        "        # Call model function and input the data tensor \n",
        "        output = None\n",
        "        # Initialize the optimizer  \n",
        "        None \n",
        "        # Compute the cost \n",
        "        loss = None\n",
        "        # Compute the back-prop\n",
        "        None\n",
        "        # Update prameters with the optmizer \n",
        "        None\n",
        "        ### END CODE HERE ###\n",
        "        if batch_idx%100==0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, \n",
        "                batch_idx*len(data), \n",
        "                len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), \n",
        "                loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqMzKBR_4xCq",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXIwfghqms6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss=0\n",
        "        correct = 0\n",
        "        for data, target in train_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output,target)\n",
        "            pred = output.data.max(1,keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "    \n",
        "        test_loss /=len(train_loader.dataset)\n",
        "        print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "            test_loss, correct, \n",
        "            len(train_loader.dataset),\n",
        "            100. * correct / len(train_loader.dataset)))\n",
        "\n",
        "        test_loss=0\n",
        "        correct = 0\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output,target)\n",
        "            pred = output.data.max(1,keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "    \n",
        "        test_loss /=len(test_loader.dataset)\n",
        "        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct, \n",
        "            len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xleeHtag4DKl",
        "colab_type": "text"
      },
      "source": [
        "### 2.3. Run the train/test function\n",
        "\n",
        "Run the following cell to train and test your model for 100 epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5McB7DF1D5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3235cc4-4abe-4767-8aa6-e6d6a9742927"
      },
      "source": [
        "for epoch in range(0,100):\n",
        "    train(epoch)\n",
        "    if epoch%100==0:\n",
        "        test()\n",
        "test()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.302638\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.302110\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.301054\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.300572\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.299504\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.298112\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 2.297964\n",
            "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 2.296887\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 2.294783\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 2.287405\n",
            "Train set: Average loss: 0.0357, Accuracy: 13808/60000 (23%)\n",
            "Test set: Average loss: 0.0359, Accuracy: 2349/10000 (23%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.286962\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.283237\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.262435\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.129799\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.886878\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.936332\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.778477\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.670924\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.741662\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.717821\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.681509\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.583248\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.672775\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.735661\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.734723\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.640545\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.656964\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.661373\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.630189\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.641044\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.705383\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.658380\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.619212\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.664862\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.596573\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.649222\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.616420\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.649786\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.558729\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 1.556320\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.574839\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.559914\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.621280\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.642738\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.625273\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.604599\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.531167\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.615369\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.619191\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.684692\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.671676\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.624834\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.601558\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.630640\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.600332\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.657858\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.596747\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.532071\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.547897\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.552167\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.577177\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.546764\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.492420\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.517072\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.499709\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.537219\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.503398\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.531528\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.544517\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.503405\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.519084\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.491006\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.484881\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.532127\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.552937\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.505885\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.497000\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.508036\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.527660\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.537952\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.515481\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.476559\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.502932\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.466215\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.494560\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.530078\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.516990\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.510810\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.501339\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.512886\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.464293\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.484999\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.476205\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.534455\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.478512\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.476566\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.504617\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.549531\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.505260\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.474611\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.498341\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 1.486640\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.523146\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 1.509528\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.504084\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.481596\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.497747\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.461955\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.528786\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 1.513301\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.466601\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 1.468105\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 1.520162\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 1.487348\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 1.497650\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 1.476213\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 1.476436\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 1.502492\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 1.522402\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 1.471753\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.497367\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 1.512327\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 1.496856\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 1.475176\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 1.465171\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 1.546996\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 1.500849\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 1.478197\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 1.466536\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 1.491557\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.499743\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 1.469781\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 1.468413\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 1.485990\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 1.473925\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 1.499822\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 1.470761\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 1.508984\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 1.477342\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 1.504285\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.493865\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 1.477895\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 1.462829\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 1.492643\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 1.493927\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 1.524212\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 1.519708\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 1.496775\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 1.478469\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 1.495977\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.477272\n",
            "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 1.513113\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 1.479639\n",
            "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 1.488785\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 1.470429\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 1.509967\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 1.495830\n",
            "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 1.462821\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 1.493958\n",
            "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 1.476060\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 1.483785\n",
            "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 1.521609\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 1.490425\n",
            "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 1.508385\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 1.463238\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 1.527523\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 1.465520\n",
            "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 1.467755\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 1.478985\n",
            "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 1.486032\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 1.516666\n",
            "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 1.477624\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 1.505673\n",
            "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 1.494467\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 1.463064\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 1.473219\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 1.465065\n",
            "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 1.535521\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 1.491462\n",
            "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 1.465318\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 1.495935\n",
            "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 1.461823\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 1.503166\n",
            "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 1.514359\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 1.487534\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 1.475225\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 1.519968\n",
            "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 1.496765\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 1.492109\n",
            "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 1.465378\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 1.501842\n",
            "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 1.487881\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 1.472676\n",
            "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 1.470780\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 1.475014\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 1.462362\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 1.478306\n",
            "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 1.488445\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 1.484220\n",
            "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 1.480452\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 1.474305\n",
            "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 1.498819\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 1.494061\n",
            "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 1.478114\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 1.462967\n",
            "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 1.467004\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 1.495392\n",
            "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 1.515525\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 1.477376\n",
            "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 1.465214\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 1.489600\n",
            "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 1.474095\n",
            "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 1.482979\n",
            "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 1.473450\n",
            "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 1.465422\n",
            "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 1.477472\n",
            "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 1.484114\n",
            "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 1.461942\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 1.488539\n",
            "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 1.479100\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 1.501505\n",
            "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 1.462355\n",
            "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 1.481328\n",
            "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 1.471148\n",
            "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 1.489266\n",
            "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 1.476875\n",
            "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 1.461309\n",
            "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 1.480210\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 1.490977\n",
            "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 1.461737\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 1.484459\n",
            "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 1.463070\n",
            "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 1.483464\n",
            "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 1.461763\n",
            "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 1.474190\n",
            "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 1.464028\n",
            "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 1.474513\n",
            "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 1.493777\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 1.467298\n",
            "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 1.497339\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 1.461657\n",
            "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 1.470193\n",
            "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 1.478237\n",
            "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 1.461753\n",
            "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 1.469803\n",
            "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 1.464423\n",
            "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 1.462265\n",
            "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 1.461702\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 1.477387\n",
            "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 1.463714\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 1.473825\n",
            "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 1.467226\n",
            "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 1.478628\n",
            "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 1.474348\n",
            "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 1.497491\n",
            "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 1.465565\n",
            "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 1.494721\n",
            "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 1.490303\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 1.502661\n",
            "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 1.472019\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 1.466510\n",
            "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 1.478196\n",
            "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 1.477137\n",
            "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 1.472246\n",
            "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 1.484651\n",
            "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 1.462949\n",
            "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 1.465119\n",
            "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 1.472031\n",
            "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 1.462615\n",
            "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 1.467104\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 1.481322\n",
            "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 1.480194\n",
            "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 1.487475\n",
            "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 1.480401\n",
            "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 1.463720\n",
            "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 1.465206\n",
            "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 1.507197\n",
            "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 1.501861\n",
            "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 1.482578\n",
            "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 1.485834\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 1.475625\n",
            "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 1.475210\n",
            "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 1.467389\n",
            "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 1.477379\n",
            "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 1.465627\n",
            "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 1.461910\n",
            "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 1.488058\n",
            "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 1.476362\n",
            "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 1.461479\n",
            "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 1.483286\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 1.461335\n",
            "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 1.477392\n",
            "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 1.462817\n",
            "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 1.481260\n",
            "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 1.500466\n",
            "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 1.462210\n",
            "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 1.465426\n",
            "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 1.461444\n",
            "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 1.492913\n",
            "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 1.461391\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 1.469790\n",
            "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 1.461621\n",
            "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 1.469456\n",
            "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 1.501379\n",
            "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 1.476879\n",
            "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 1.479514\n",
            "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 1.468124\n",
            "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 1.464452\n",
            "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 1.469854\n",
            "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 1.463871\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 1.487259\n",
            "Train Epoch: 31 [6400/60000 (11%)]\tLoss: 1.481460\n",
            "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 1.478783\n",
            "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 1.499223\n",
            "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 1.479452\n",
            "Train Epoch: 31 [32000/60000 (53%)]\tLoss: 1.462329\n",
            "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 1.462083\n",
            "Train Epoch: 31 [44800/60000 (75%)]\tLoss: 1.463326\n",
            "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 1.485577\n",
            "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 1.479306\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 1.461513\n",
            "Train Epoch: 32 [6400/60000 (11%)]\tLoss: 1.494079\n",
            "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 1.476944\n",
            "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 1.462413\n",
            "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 1.494387\n",
            "Train Epoch: 32 [32000/60000 (53%)]\tLoss: 1.469604\n",
            "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 1.464277\n",
            "Train Epoch: 32 [44800/60000 (75%)]\tLoss: 1.476190\n",
            "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 1.480264\n",
            "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 1.491719\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 1.472472\n",
            "Train Epoch: 33 [6400/60000 (11%)]\tLoss: 1.478428\n",
            "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 1.496671\n",
            "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 1.461578\n",
            "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 1.468859\n",
            "Train Epoch: 33 [32000/60000 (53%)]\tLoss: 1.469870\n",
            "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 1.461985\n",
            "Train Epoch: 33 [44800/60000 (75%)]\tLoss: 1.463642\n",
            "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 1.476899\n",
            "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 1.468860\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 1.461174\n",
            "Train Epoch: 34 [6400/60000 (11%)]\tLoss: 1.464682\n",
            "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 1.465365\n",
            "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 1.462387\n",
            "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 1.471737\n",
            "Train Epoch: 34 [32000/60000 (53%)]\tLoss: 1.494391\n",
            "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 1.477014\n",
            "Train Epoch: 34 [44800/60000 (75%)]\tLoss: 1.464749\n",
            "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 1.461618\n",
            "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 1.468323\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 1.477552\n",
            "Train Epoch: 35 [6400/60000 (11%)]\tLoss: 1.500922\n",
            "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 1.476864\n",
            "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 1.462296\n",
            "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 1.477036\n",
            "Train Epoch: 35 [32000/60000 (53%)]\tLoss: 1.473670\n",
            "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 1.504106\n",
            "Train Epoch: 35 [44800/60000 (75%)]\tLoss: 1.466197\n",
            "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 1.461393\n",
            "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 1.461381\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 1.462271\n",
            "Train Epoch: 36 [6400/60000 (11%)]\tLoss: 1.464829\n",
            "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 1.467891\n",
            "Train Epoch: 36 [19200/60000 (32%)]\tLoss: 1.469492\n",
            "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 1.482579\n",
            "Train Epoch: 36 [32000/60000 (53%)]\tLoss: 1.481460\n",
            "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 1.461788\n",
            "Train Epoch: 36 [44800/60000 (75%)]\tLoss: 1.469837\n",
            "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 1.478765\n",
            "Train Epoch: 36 [57600/60000 (96%)]\tLoss: 1.461192\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 1.463094\n",
            "Train Epoch: 37 [6400/60000 (11%)]\tLoss: 1.461727\n",
            "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 1.467794\n",
            "Train Epoch: 37 [19200/60000 (32%)]\tLoss: 1.470900\n",
            "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 1.461564\n",
            "Train Epoch: 37 [32000/60000 (53%)]\tLoss: 1.477903\n",
            "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 1.463209\n",
            "Train Epoch: 37 [44800/60000 (75%)]\tLoss: 1.478172\n",
            "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 1.462939\n",
            "Train Epoch: 37 [57600/60000 (96%)]\tLoss: 1.463498\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 1.461338\n",
            "Train Epoch: 38 [6400/60000 (11%)]\tLoss: 1.462040\n",
            "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 1.464221\n",
            "Train Epoch: 38 [19200/60000 (32%)]\tLoss: 1.464142\n",
            "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 1.482330\n",
            "Train Epoch: 38 [32000/60000 (53%)]\tLoss: 1.479118\n",
            "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 1.477047\n",
            "Train Epoch: 38 [44800/60000 (75%)]\tLoss: 1.468869\n",
            "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 1.474563\n",
            "Train Epoch: 38 [57600/60000 (96%)]\tLoss: 1.461864\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 1.478133\n",
            "Train Epoch: 39 [6400/60000 (11%)]\tLoss: 1.467983\n",
            "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 1.469074\n",
            "Train Epoch: 39 [19200/60000 (32%)]\tLoss: 1.464722\n",
            "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 1.461356\n",
            "Train Epoch: 39 [32000/60000 (53%)]\tLoss: 1.461753\n",
            "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 1.462684\n",
            "Train Epoch: 39 [44800/60000 (75%)]\tLoss: 1.464302\n",
            "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 1.467084\n",
            "Train Epoch: 39 [57600/60000 (96%)]\tLoss: 1.461231\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 1.469451\n",
            "Train Epoch: 40 [6400/60000 (11%)]\tLoss: 1.499596\n",
            "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 1.469392\n",
            "Train Epoch: 40 [19200/60000 (32%)]\tLoss: 1.470484\n",
            "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 1.480952\n",
            "Train Epoch: 40 [32000/60000 (53%)]\tLoss: 1.461226\n",
            "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 1.476381\n",
            "Train Epoch: 40 [44800/60000 (75%)]\tLoss: 1.461584\n",
            "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 1.463158\n",
            "Train Epoch: 40 [57600/60000 (96%)]\tLoss: 1.462516\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 1.495721\n",
            "Train Epoch: 41 [6400/60000 (11%)]\tLoss: 1.462708\n",
            "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 1.478526\n",
            "Train Epoch: 41 [19200/60000 (32%)]\tLoss: 1.466860\n",
            "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 1.468828\n",
            "Train Epoch: 41 [32000/60000 (53%)]\tLoss: 1.473270\n",
            "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 1.462091\n",
            "Train Epoch: 41 [44800/60000 (75%)]\tLoss: 1.461626\n",
            "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 1.464249\n",
            "Train Epoch: 41 [57600/60000 (96%)]\tLoss: 1.467796\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 1.476125\n",
            "Train Epoch: 42 [6400/60000 (11%)]\tLoss: 1.462243\n",
            "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 1.492834\n",
            "Train Epoch: 42 [19200/60000 (32%)]\tLoss: 1.462578\n",
            "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 1.461535\n",
            "Train Epoch: 42 [32000/60000 (53%)]\tLoss: 1.462538\n",
            "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 1.466372\n",
            "Train Epoch: 42 [44800/60000 (75%)]\tLoss: 1.465557\n",
            "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 1.464351\n",
            "Train Epoch: 42 [57600/60000 (96%)]\tLoss: 1.461707\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 1.461642\n",
            "Train Epoch: 43 [6400/60000 (11%)]\tLoss: 1.461480\n",
            "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 1.479777\n",
            "Train Epoch: 43 [19200/60000 (32%)]\tLoss: 1.462301\n",
            "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 1.461308\n",
            "Train Epoch: 43 [32000/60000 (53%)]\tLoss: 1.462176\n",
            "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 1.480211\n",
            "Train Epoch: 43 [44800/60000 (75%)]\tLoss: 1.463997\n",
            "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 1.475841\n",
            "Train Epoch: 43 [57600/60000 (96%)]\tLoss: 1.462199\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 1.464507\n",
            "Train Epoch: 44 [6400/60000 (11%)]\tLoss: 1.478017\n",
            "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 1.476281\n",
            "Train Epoch: 44 [19200/60000 (32%)]\tLoss: 1.464332\n",
            "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 1.463814\n",
            "Train Epoch: 44 [32000/60000 (53%)]\tLoss: 1.462546\n",
            "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 1.461720\n",
            "Train Epoch: 44 [44800/60000 (75%)]\tLoss: 1.461714\n",
            "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 1.474210\n",
            "Train Epoch: 44 [57600/60000 (96%)]\tLoss: 1.475003\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 1.468021\n",
            "Train Epoch: 45 [6400/60000 (11%)]\tLoss: 1.461506\n",
            "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 1.464971\n",
            "Train Epoch: 45 [19200/60000 (32%)]\tLoss: 1.465022\n",
            "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 1.462203\n",
            "Train Epoch: 45 [32000/60000 (53%)]\tLoss: 1.476940\n",
            "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 1.462252\n",
            "Train Epoch: 45 [44800/60000 (75%)]\tLoss: 1.463629\n",
            "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 1.463096\n",
            "Train Epoch: 45 [57600/60000 (96%)]\tLoss: 1.477158\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 1.493531\n",
            "Train Epoch: 46 [6400/60000 (11%)]\tLoss: 1.461873\n",
            "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 1.462245\n",
            "Train Epoch: 46 [19200/60000 (32%)]\tLoss: 1.485168\n",
            "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 1.477026\n",
            "Train Epoch: 46 [32000/60000 (53%)]\tLoss: 1.461288\n",
            "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 1.462037\n",
            "Train Epoch: 46 [44800/60000 (75%)]\tLoss: 1.476726\n",
            "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 1.469666\n",
            "Train Epoch: 46 [57600/60000 (96%)]\tLoss: 1.461409\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 1.492088\n",
            "Train Epoch: 47 [6400/60000 (11%)]\tLoss: 1.462452\n",
            "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 1.462840\n",
            "Train Epoch: 47 [19200/60000 (32%)]\tLoss: 1.493974\n",
            "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 1.462919\n",
            "Train Epoch: 47 [32000/60000 (53%)]\tLoss: 1.461615\n",
            "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 1.477693\n",
            "Train Epoch: 47 [44800/60000 (75%)]\tLoss: 1.466219\n",
            "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 1.463673\n",
            "Train Epoch: 47 [57600/60000 (96%)]\tLoss: 1.461807\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 1.467287\n",
            "Train Epoch: 48 [6400/60000 (11%)]\tLoss: 1.492384\n",
            "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 1.462911\n",
            "Train Epoch: 48 [19200/60000 (32%)]\tLoss: 1.463381\n",
            "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 1.461324\n",
            "Train Epoch: 48 [32000/60000 (53%)]\tLoss: 1.464834\n",
            "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 1.469835\n",
            "Train Epoch: 48 [44800/60000 (75%)]\tLoss: 1.461359\n",
            "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 1.464232\n",
            "Train Epoch: 48 [57600/60000 (96%)]\tLoss: 1.478047\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 1.477037\n",
            "Train Epoch: 49 [6400/60000 (11%)]\tLoss: 1.461790\n",
            "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 1.492680\n",
            "Train Epoch: 49 [19200/60000 (32%)]\tLoss: 1.461744\n",
            "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 1.461514\n",
            "Train Epoch: 49 [32000/60000 (53%)]\tLoss: 1.461351\n",
            "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 1.461314\n",
            "Train Epoch: 49 [44800/60000 (75%)]\tLoss: 1.461799\n",
            "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 1.461894\n",
            "Train Epoch: 49 [57600/60000 (96%)]\tLoss: 1.492547\n",
            "Train Epoch: 50 [0/60000 (0%)]\tLoss: 1.477874\n",
            "Train Epoch: 50 [6400/60000 (11%)]\tLoss: 1.461380\n",
            "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 1.475040\n",
            "Train Epoch: 50 [19200/60000 (32%)]\tLoss: 1.462639\n",
            "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 1.477774\n",
            "Train Epoch: 50 [32000/60000 (53%)]\tLoss: 1.462556\n",
            "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 1.477903\n",
            "Train Epoch: 50 [44800/60000 (75%)]\tLoss: 1.471252\n",
            "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 1.461212\n",
            "Train Epoch: 50 [57600/60000 (96%)]\tLoss: 1.463408\n",
            "Train Epoch: 51 [0/60000 (0%)]\tLoss: 1.476256\n",
            "Train Epoch: 51 [6400/60000 (11%)]\tLoss: 1.463119\n",
            "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 1.461280\n",
            "Train Epoch: 51 [19200/60000 (32%)]\tLoss: 1.464459\n",
            "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 1.461313\n",
            "Train Epoch: 51 [32000/60000 (53%)]\tLoss: 1.464232\n",
            "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 1.461366\n",
            "Train Epoch: 51 [44800/60000 (75%)]\tLoss: 1.480955\n",
            "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 1.461406\n",
            "Train Epoch: 51 [57600/60000 (96%)]\tLoss: 1.461690\n",
            "Train Epoch: 52 [0/60000 (0%)]\tLoss: 1.476898\n",
            "Train Epoch: 52 [6400/60000 (11%)]\tLoss: 1.464514\n",
            "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 1.461166\n",
            "Train Epoch: 52 [19200/60000 (32%)]\tLoss: 1.461409\n",
            "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 1.463572\n",
            "Train Epoch: 52 [32000/60000 (53%)]\tLoss: 1.477141\n",
            "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 1.461242\n",
            "Train Epoch: 52 [44800/60000 (75%)]\tLoss: 1.462502\n",
            "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 1.464958\n",
            "Train Epoch: 52 [57600/60000 (96%)]\tLoss: 1.462747\n",
            "Train Epoch: 53 [0/60000 (0%)]\tLoss: 1.461537\n",
            "Train Epoch: 53 [6400/60000 (11%)]\tLoss: 1.476884\n",
            "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 1.461653\n",
            "Train Epoch: 53 [19200/60000 (32%)]\tLoss: 1.461620\n",
            "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 1.492537\n",
            "Train Epoch: 53 [32000/60000 (53%)]\tLoss: 1.476882\n",
            "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 1.463223\n",
            "Train Epoch: 53 [44800/60000 (75%)]\tLoss: 1.463114\n",
            "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 1.467728\n",
            "Train Epoch: 53 [57600/60000 (96%)]\tLoss: 1.461501\n",
            "Train Epoch: 54 [0/60000 (0%)]\tLoss: 1.466804\n",
            "Train Epoch: 54 [6400/60000 (11%)]\tLoss: 1.461293\n",
            "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 1.463317\n",
            "Train Epoch: 54 [19200/60000 (32%)]\tLoss: 1.465728\n",
            "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 1.476966\n",
            "Train Epoch: 54 [32000/60000 (53%)]\tLoss: 1.477022\n",
            "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 1.481291\n",
            "Train Epoch: 54 [44800/60000 (75%)]\tLoss: 1.462197\n",
            "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 1.461885\n",
            "Train Epoch: 54 [57600/60000 (96%)]\tLoss: 1.476929\n",
            "Train Epoch: 55 [0/60000 (0%)]\tLoss: 1.461533\n",
            "Train Epoch: 55 [6400/60000 (11%)]\tLoss: 1.478198\n",
            "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 1.466233\n",
            "Train Epoch: 55 [19200/60000 (32%)]\tLoss: 1.461482\n",
            "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 1.465748\n",
            "Train Epoch: 55 [32000/60000 (53%)]\tLoss: 1.461317\n",
            "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 1.461348\n",
            "Train Epoch: 55 [44800/60000 (75%)]\tLoss: 1.464839\n",
            "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 1.463199\n",
            "Train Epoch: 55 [57600/60000 (96%)]\tLoss: 1.462149\n",
            "Train Epoch: 56 [0/60000 (0%)]\tLoss: 1.462590\n",
            "Train Epoch: 56 [6400/60000 (11%)]\tLoss: 1.461995\n",
            "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 1.463524\n",
            "Train Epoch: 56 [19200/60000 (32%)]\tLoss: 1.472291\n",
            "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 1.463866\n",
            "Train Epoch: 56 [32000/60000 (53%)]\tLoss: 1.492930\n",
            "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 1.477084\n",
            "Train Epoch: 56 [44800/60000 (75%)]\tLoss: 1.461889\n",
            "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 1.463565\n",
            "Train Epoch: 56 [57600/60000 (96%)]\tLoss: 1.491887\n",
            "Train Epoch: 57 [0/60000 (0%)]\tLoss: 1.461446\n",
            "Train Epoch: 57 [6400/60000 (11%)]\tLoss: 1.475522\n",
            "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 1.461744\n",
            "Train Epoch: 57 [19200/60000 (32%)]\tLoss: 1.476967\n",
            "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 1.497470\n",
            "Train Epoch: 57 [32000/60000 (53%)]\tLoss: 1.476897\n",
            "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 1.477074\n",
            "Train Epoch: 57 [44800/60000 (75%)]\tLoss: 1.462600\n",
            "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 1.477447\n",
            "Train Epoch: 57 [57600/60000 (96%)]\tLoss: 1.463500\n",
            "Train Epoch: 58 [0/60000 (0%)]\tLoss: 1.462590\n",
            "Train Epoch: 58 [6400/60000 (11%)]\tLoss: 1.476773\n",
            "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 1.481013\n",
            "Train Epoch: 58 [19200/60000 (32%)]\tLoss: 1.462016\n",
            "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 1.462707\n",
            "Train Epoch: 58 [32000/60000 (53%)]\tLoss: 1.463827\n",
            "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 1.462482\n",
            "Train Epoch: 58 [44800/60000 (75%)]\tLoss: 1.461252\n",
            "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 1.491625\n",
            "Train Epoch: 58 [57600/60000 (96%)]\tLoss: 1.478676\n",
            "Train Epoch: 59 [0/60000 (0%)]\tLoss: 1.461318\n",
            "Train Epoch: 59 [6400/60000 (11%)]\tLoss: 1.464989\n",
            "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 1.508087\n",
            "Train Epoch: 59 [19200/60000 (32%)]\tLoss: 1.461708\n",
            "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 1.462225\n",
            "Train Epoch: 59 [32000/60000 (53%)]\tLoss: 1.465073\n",
            "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 1.461788\n",
            "Train Epoch: 59 [44800/60000 (75%)]\tLoss: 1.462347\n",
            "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 1.461263\n",
            "Train Epoch: 59 [57600/60000 (96%)]\tLoss: 1.463192\n",
            "Train Epoch: 60 [0/60000 (0%)]\tLoss: 1.462553\n",
            "Train Epoch: 60 [6400/60000 (11%)]\tLoss: 1.461884\n",
            "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 1.492377\n",
            "Train Epoch: 60 [19200/60000 (32%)]\tLoss: 1.492635\n",
            "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 1.462217\n",
            "Train Epoch: 60 [32000/60000 (53%)]\tLoss: 1.461572\n",
            "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 1.461160\n",
            "Train Epoch: 60 [44800/60000 (75%)]\tLoss: 1.461413\n",
            "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 1.462654\n",
            "Train Epoch: 60 [57600/60000 (96%)]\tLoss: 1.497625\n",
            "Train Epoch: 61 [0/60000 (0%)]\tLoss: 1.461212\n",
            "Train Epoch: 61 [6400/60000 (11%)]\tLoss: 1.462589\n",
            "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 1.476864\n",
            "Train Epoch: 61 [19200/60000 (32%)]\tLoss: 1.463445\n",
            "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 1.461406\n",
            "Train Epoch: 61 [32000/60000 (53%)]\tLoss: 1.461434\n",
            "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 1.461193\n",
            "Train Epoch: 61 [44800/60000 (75%)]\tLoss: 1.461266\n",
            "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 1.461343\n",
            "Train Epoch: 61 [57600/60000 (96%)]\tLoss: 1.462291\n",
            "Train Epoch: 62 [0/60000 (0%)]\tLoss: 1.461670\n",
            "Train Epoch: 62 [6400/60000 (11%)]\tLoss: 1.478052\n",
            "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 1.462001\n",
            "Train Epoch: 62 [19200/60000 (32%)]\tLoss: 1.499038\n",
            "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 1.463054\n",
            "Train Epoch: 62 [32000/60000 (53%)]\tLoss: 1.466317\n",
            "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 1.461286\n",
            "Train Epoch: 62 [44800/60000 (75%)]\tLoss: 1.461183\n",
            "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 1.467717\n",
            "Train Epoch: 62 [57600/60000 (96%)]\tLoss: 1.472014\n",
            "Train Epoch: 63 [0/60000 (0%)]\tLoss: 1.482882\n",
            "Train Epoch: 63 [6400/60000 (11%)]\tLoss: 1.465743\n",
            "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 1.490875\n",
            "Train Epoch: 63 [19200/60000 (32%)]\tLoss: 1.477883\n",
            "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 1.461203\n",
            "Train Epoch: 63 [32000/60000 (53%)]\tLoss: 1.461616\n",
            "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 1.461169\n",
            "Train Epoch: 63 [44800/60000 (75%)]\tLoss: 1.461752\n",
            "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 1.461463\n",
            "Train Epoch: 63 [57600/60000 (96%)]\tLoss: 1.461469\n",
            "Train Epoch: 64 [0/60000 (0%)]\tLoss: 1.461297\n",
            "Train Epoch: 64 [6400/60000 (11%)]\tLoss: 1.476804\n",
            "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 1.461166\n",
            "Train Epoch: 64 [19200/60000 (32%)]\tLoss: 1.461838\n",
            "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 1.462901\n",
            "Train Epoch: 64 [32000/60000 (53%)]\tLoss: 1.461169\n",
            "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 1.461253\n",
            "Train Epoch: 64 [44800/60000 (75%)]\tLoss: 1.461779\n",
            "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 1.465941\n",
            "Train Epoch: 64 [57600/60000 (96%)]\tLoss: 1.461245\n",
            "Train Epoch: 65 [0/60000 (0%)]\tLoss: 1.463192\n",
            "Train Epoch: 65 [6400/60000 (11%)]\tLoss: 1.461617\n",
            "Train Epoch: 65 [12800/60000 (21%)]\tLoss: 1.461697\n",
            "Train Epoch: 65 [19200/60000 (32%)]\tLoss: 1.461485\n",
            "Train Epoch: 65 [25600/60000 (43%)]\tLoss: 1.477125\n",
            "Train Epoch: 65 [32000/60000 (53%)]\tLoss: 1.476936\n",
            "Train Epoch: 65 [38400/60000 (64%)]\tLoss: 1.461241\n",
            "Train Epoch: 65 [44800/60000 (75%)]\tLoss: 1.477278\n",
            "Train Epoch: 65 [51200/60000 (85%)]\tLoss: 1.461475\n",
            "Train Epoch: 65 [57600/60000 (96%)]\tLoss: 1.461188\n",
            "Train Epoch: 66 [0/60000 (0%)]\tLoss: 1.461215\n",
            "Train Epoch: 66 [6400/60000 (11%)]\tLoss: 1.465763\n",
            "Train Epoch: 66 [12800/60000 (21%)]\tLoss: 1.461168\n",
            "Train Epoch: 66 [19200/60000 (32%)]\tLoss: 1.461194\n",
            "Train Epoch: 66 [25600/60000 (43%)]\tLoss: 1.461181\n",
            "Train Epoch: 66 [32000/60000 (53%)]\tLoss: 1.461300\n",
            "Train Epoch: 66 [38400/60000 (64%)]\tLoss: 1.461451\n",
            "Train Epoch: 66 [44800/60000 (75%)]\tLoss: 1.464337\n",
            "Train Epoch: 66 [51200/60000 (85%)]\tLoss: 1.461212\n",
            "Train Epoch: 66 [57600/60000 (96%)]\tLoss: 1.461220\n",
            "Train Epoch: 67 [0/60000 (0%)]\tLoss: 1.461271\n",
            "Train Epoch: 67 [6400/60000 (11%)]\tLoss: 1.461219\n",
            "Train Epoch: 67 [12800/60000 (21%)]\tLoss: 1.461438\n",
            "Train Epoch: 67 [19200/60000 (32%)]\tLoss: 1.461334\n",
            "Train Epoch: 67 [25600/60000 (43%)]\tLoss: 1.461347\n",
            "Train Epoch: 67 [32000/60000 (53%)]\tLoss: 1.462455\n",
            "Train Epoch: 67 [38400/60000 (64%)]\tLoss: 1.461514\n",
            "Train Epoch: 67 [44800/60000 (75%)]\tLoss: 1.461503\n",
            "Train Epoch: 67 [51200/60000 (85%)]\tLoss: 1.463226\n",
            "Train Epoch: 67 [57600/60000 (96%)]\tLoss: 1.461676\n",
            "Train Epoch: 68 [0/60000 (0%)]\tLoss: 1.462847\n",
            "Train Epoch: 68 [6400/60000 (11%)]\tLoss: 1.461320\n",
            "Train Epoch: 68 [12800/60000 (21%)]\tLoss: 1.461265\n",
            "Train Epoch: 68 [19200/60000 (32%)]\tLoss: 1.476378\n",
            "Train Epoch: 68 [25600/60000 (43%)]\tLoss: 1.461511\n",
            "Train Epoch: 68 [32000/60000 (53%)]\tLoss: 1.470591\n",
            "Train Epoch: 68 [38400/60000 (64%)]\tLoss: 1.462222\n",
            "Train Epoch: 68 [44800/60000 (75%)]\tLoss: 1.462409\n",
            "Train Epoch: 68 [51200/60000 (85%)]\tLoss: 1.479056\n",
            "Train Epoch: 68 [57600/60000 (96%)]\tLoss: 1.462983\n",
            "Train Epoch: 69 [0/60000 (0%)]\tLoss: 1.461645\n",
            "Train Epoch: 69 [6400/60000 (11%)]\tLoss: 1.473282\n",
            "Train Epoch: 69 [12800/60000 (21%)]\tLoss: 1.475258\n",
            "Train Epoch: 69 [19200/60000 (32%)]\tLoss: 1.461675\n",
            "Train Epoch: 69 [25600/60000 (43%)]\tLoss: 1.464236\n",
            "Train Epoch: 69 [32000/60000 (53%)]\tLoss: 1.477214\n",
            "Train Epoch: 69 [38400/60000 (64%)]\tLoss: 1.461155\n",
            "Train Epoch: 69 [44800/60000 (75%)]\tLoss: 1.461296\n",
            "Train Epoch: 69 [51200/60000 (85%)]\tLoss: 1.487735\n",
            "Train Epoch: 69 [57600/60000 (96%)]\tLoss: 1.464615\n",
            "Train Epoch: 70 [0/60000 (0%)]\tLoss: 1.461226\n",
            "Train Epoch: 70 [6400/60000 (11%)]\tLoss: 1.461345\n",
            "Train Epoch: 70 [12800/60000 (21%)]\tLoss: 1.461286\n",
            "Train Epoch: 70 [19200/60000 (32%)]\tLoss: 1.461402\n",
            "Train Epoch: 70 [25600/60000 (43%)]\tLoss: 1.462198\n",
            "Train Epoch: 70 [32000/60000 (53%)]\tLoss: 1.461158\n",
            "Train Epoch: 70 [38400/60000 (64%)]\tLoss: 1.461891\n",
            "Train Epoch: 70 [44800/60000 (75%)]\tLoss: 1.461500\n",
            "Train Epoch: 70 [51200/60000 (85%)]\tLoss: 1.461368\n",
            "Train Epoch: 70 [57600/60000 (96%)]\tLoss: 1.466989\n",
            "Train Epoch: 71 [0/60000 (0%)]\tLoss: 1.461546\n",
            "Train Epoch: 71 [6400/60000 (11%)]\tLoss: 1.463111\n",
            "Train Epoch: 71 [12800/60000 (21%)]\tLoss: 1.477689\n",
            "Train Epoch: 71 [19200/60000 (32%)]\tLoss: 1.461925\n",
            "Train Epoch: 71 [25600/60000 (43%)]\tLoss: 1.461451\n",
            "Train Epoch: 71 [32000/60000 (53%)]\tLoss: 1.461825\n",
            "Train Epoch: 71 [38400/60000 (64%)]\tLoss: 1.463670\n",
            "Train Epoch: 71 [44800/60000 (75%)]\tLoss: 1.471387\n",
            "Train Epoch: 71 [51200/60000 (85%)]\tLoss: 1.461163\n",
            "Train Epoch: 71 [57600/60000 (96%)]\tLoss: 1.461581\n",
            "Train Epoch: 72 [0/60000 (0%)]\tLoss: 1.461167\n",
            "Train Epoch: 72 [6400/60000 (11%)]\tLoss: 1.461756\n",
            "Train Epoch: 72 [12800/60000 (21%)]\tLoss: 1.461441\n",
            "Train Epoch: 72 [19200/60000 (32%)]\tLoss: 1.461236\n",
            "Train Epoch: 72 [25600/60000 (43%)]\tLoss: 1.463850\n",
            "Train Epoch: 72 [32000/60000 (53%)]\tLoss: 1.461166\n",
            "Train Epoch: 72 [38400/60000 (64%)]\tLoss: 1.463879\n",
            "Train Epoch: 72 [44800/60000 (75%)]\tLoss: 1.462046\n",
            "Train Epoch: 72 [51200/60000 (85%)]\tLoss: 1.461212\n",
            "Train Epoch: 72 [57600/60000 (96%)]\tLoss: 1.461358\n",
            "Train Epoch: 73 [0/60000 (0%)]\tLoss: 1.461303\n",
            "Train Epoch: 73 [6400/60000 (11%)]\tLoss: 1.461642\n",
            "Train Epoch: 73 [12800/60000 (21%)]\tLoss: 1.461235\n",
            "Train Epoch: 73 [19200/60000 (32%)]\tLoss: 1.461156\n",
            "Train Epoch: 73 [25600/60000 (43%)]\tLoss: 1.469705\n",
            "Train Epoch: 73 [32000/60000 (53%)]\tLoss: 1.476104\n",
            "Train Epoch: 73 [38400/60000 (64%)]\tLoss: 1.461657\n",
            "Train Epoch: 73 [44800/60000 (75%)]\tLoss: 1.464918\n",
            "Train Epoch: 73 [51200/60000 (85%)]\tLoss: 1.461389\n",
            "Train Epoch: 73 [57600/60000 (96%)]\tLoss: 1.461630\n",
            "Train Epoch: 74 [0/60000 (0%)]\tLoss: 1.479080\n",
            "Train Epoch: 74 [6400/60000 (11%)]\tLoss: 1.461203\n",
            "Train Epoch: 74 [12800/60000 (21%)]\tLoss: 1.461308\n",
            "Train Epoch: 74 [19200/60000 (32%)]\tLoss: 1.464458\n",
            "Train Epoch: 74 [25600/60000 (43%)]\tLoss: 1.464135\n",
            "Train Epoch: 74 [32000/60000 (53%)]\tLoss: 1.476901\n",
            "Train Epoch: 74 [38400/60000 (64%)]\tLoss: 1.461175\n",
            "Train Epoch: 74 [44800/60000 (75%)]\tLoss: 1.461187\n",
            "Train Epoch: 74 [51200/60000 (85%)]\tLoss: 1.477146\n",
            "Train Epoch: 74 [57600/60000 (96%)]\tLoss: 1.461423\n",
            "Train Epoch: 75 [0/60000 (0%)]\tLoss: 1.464692\n",
            "Train Epoch: 75 [6400/60000 (11%)]\tLoss: 1.461227\n",
            "Train Epoch: 75 [12800/60000 (21%)]\tLoss: 1.461442\n",
            "Train Epoch: 75 [19200/60000 (32%)]\tLoss: 1.461341\n",
            "Train Epoch: 75 [25600/60000 (43%)]\tLoss: 1.461410\n",
            "Train Epoch: 75 [32000/60000 (53%)]\tLoss: 1.476692\n",
            "Train Epoch: 75 [38400/60000 (64%)]\tLoss: 1.461311\n",
            "Train Epoch: 75 [44800/60000 (75%)]\tLoss: 1.463648\n",
            "Train Epoch: 75 [51200/60000 (85%)]\tLoss: 1.463168\n",
            "Train Epoch: 75 [57600/60000 (96%)]\tLoss: 1.462281\n",
            "Train Epoch: 76 [0/60000 (0%)]\tLoss: 1.463003\n",
            "Train Epoch: 76 [6400/60000 (11%)]\tLoss: 1.461287\n",
            "Train Epoch: 76 [12800/60000 (21%)]\tLoss: 1.461572\n",
            "Train Epoch: 76 [19200/60000 (32%)]\tLoss: 1.461627\n",
            "Train Epoch: 76 [25600/60000 (43%)]\tLoss: 1.461217\n",
            "Train Epoch: 76 [32000/60000 (53%)]\tLoss: 1.463194\n",
            "Train Epoch: 76 [38400/60000 (64%)]\tLoss: 1.462220\n",
            "Train Epoch: 76 [44800/60000 (75%)]\tLoss: 1.461158\n",
            "Train Epoch: 76 [51200/60000 (85%)]\tLoss: 1.461593\n",
            "Train Epoch: 76 [57600/60000 (96%)]\tLoss: 1.461402\n",
            "Train Epoch: 77 [0/60000 (0%)]\tLoss: 1.461401\n",
            "Train Epoch: 77 [6400/60000 (11%)]\tLoss: 1.461598\n",
            "Train Epoch: 77 [12800/60000 (21%)]\tLoss: 1.461383\n",
            "Train Epoch: 77 [19200/60000 (32%)]\tLoss: 1.465203\n",
            "Train Epoch: 77 [25600/60000 (43%)]\tLoss: 1.461280\n",
            "Train Epoch: 77 [32000/60000 (53%)]\tLoss: 1.465012\n",
            "Train Epoch: 77 [38400/60000 (64%)]\tLoss: 1.463489\n",
            "Train Epoch: 77 [44800/60000 (75%)]\tLoss: 1.461288\n",
            "Train Epoch: 77 [51200/60000 (85%)]\tLoss: 1.461220\n",
            "Train Epoch: 77 [57600/60000 (96%)]\tLoss: 1.477158\n",
            "Train Epoch: 78 [0/60000 (0%)]\tLoss: 1.461192\n",
            "Train Epoch: 78 [6400/60000 (11%)]\tLoss: 1.461164\n",
            "Train Epoch: 78 [12800/60000 (21%)]\tLoss: 1.461318\n",
            "Train Epoch: 78 [19200/60000 (32%)]\tLoss: 1.461180\n",
            "Train Epoch: 78 [25600/60000 (43%)]\tLoss: 1.461292\n",
            "Train Epoch: 78 [32000/60000 (53%)]\tLoss: 1.461212\n",
            "Train Epoch: 78 [38400/60000 (64%)]\tLoss: 1.477677\n",
            "Train Epoch: 78 [44800/60000 (75%)]\tLoss: 1.461200\n",
            "Train Epoch: 78 [51200/60000 (85%)]\tLoss: 1.461155\n",
            "Train Epoch: 78 [57600/60000 (96%)]\tLoss: 1.475098\n",
            "Train Epoch: 79 [0/60000 (0%)]\tLoss: 1.461193\n",
            "Train Epoch: 79 [6400/60000 (11%)]\tLoss: 1.461227\n",
            "Train Epoch: 79 [12800/60000 (21%)]\tLoss: 1.477073\n",
            "Train Epoch: 79 [19200/60000 (32%)]\tLoss: 1.462052\n",
            "Train Epoch: 79 [25600/60000 (43%)]\tLoss: 1.461960\n",
            "Train Epoch: 79 [32000/60000 (53%)]\tLoss: 1.461874\n",
            "Train Epoch: 79 [38400/60000 (64%)]\tLoss: 1.461322\n",
            "Train Epoch: 79 [44800/60000 (75%)]\tLoss: 1.461272\n",
            "Train Epoch: 79 [51200/60000 (85%)]\tLoss: 1.461869\n",
            "Train Epoch: 79 [57600/60000 (96%)]\tLoss: 1.463713\n",
            "Train Epoch: 80 [0/60000 (0%)]\tLoss: 1.461434\n",
            "Train Epoch: 80 [6400/60000 (11%)]\tLoss: 1.461653\n",
            "Train Epoch: 80 [12800/60000 (21%)]\tLoss: 1.461336\n",
            "Train Epoch: 80 [19200/60000 (32%)]\tLoss: 1.462077\n",
            "Train Epoch: 80 [25600/60000 (43%)]\tLoss: 1.499539\n",
            "Train Epoch: 80 [32000/60000 (53%)]\tLoss: 1.461489\n",
            "Train Epoch: 80 [38400/60000 (64%)]\tLoss: 1.461266\n",
            "Train Epoch: 80 [44800/60000 (75%)]\tLoss: 1.465235\n",
            "Train Epoch: 80 [51200/60000 (85%)]\tLoss: 1.466455\n",
            "Train Epoch: 80 [57600/60000 (96%)]\tLoss: 1.461471\n",
            "Train Epoch: 81 [0/60000 (0%)]\tLoss: 1.461418\n",
            "Train Epoch: 81 [6400/60000 (11%)]\tLoss: 1.461251\n",
            "Train Epoch: 81 [12800/60000 (21%)]\tLoss: 1.461347\n",
            "Train Epoch: 81 [19200/60000 (32%)]\tLoss: 1.478811\n",
            "Train Epoch: 81 [25600/60000 (43%)]\tLoss: 1.461169\n",
            "Train Epoch: 81 [32000/60000 (53%)]\tLoss: 1.461385\n",
            "Train Epoch: 81 [38400/60000 (64%)]\tLoss: 1.476318\n",
            "Train Epoch: 81 [44800/60000 (75%)]\tLoss: 1.476828\n",
            "Train Epoch: 81 [51200/60000 (85%)]\tLoss: 1.461720\n",
            "Train Epoch: 81 [57600/60000 (96%)]\tLoss: 1.461246\n",
            "Train Epoch: 82 [0/60000 (0%)]\tLoss: 1.466271\n",
            "Train Epoch: 82 [6400/60000 (11%)]\tLoss: 1.461926\n",
            "Train Epoch: 82 [12800/60000 (21%)]\tLoss: 1.461317\n",
            "Train Epoch: 82 [19200/60000 (32%)]\tLoss: 1.461491\n",
            "Train Epoch: 82 [25600/60000 (43%)]\tLoss: 1.461328\n",
            "Train Epoch: 82 [32000/60000 (53%)]\tLoss: 1.477700\n",
            "Train Epoch: 82 [38400/60000 (64%)]\tLoss: 1.463661\n",
            "Train Epoch: 82 [44800/60000 (75%)]\tLoss: 1.462349\n",
            "Train Epoch: 82 [51200/60000 (85%)]\tLoss: 1.461506\n",
            "Train Epoch: 82 [57600/60000 (96%)]\tLoss: 1.461220\n",
            "Train Epoch: 83 [0/60000 (0%)]\tLoss: 1.464660\n",
            "Train Epoch: 83 [6400/60000 (11%)]\tLoss: 1.461298\n",
            "Train Epoch: 83 [12800/60000 (21%)]\tLoss: 1.461473\n",
            "Train Epoch: 83 [19200/60000 (32%)]\tLoss: 1.461166\n",
            "Train Epoch: 83 [25600/60000 (43%)]\tLoss: 1.475674\n",
            "Train Epoch: 83 [32000/60000 (53%)]\tLoss: 1.461270\n",
            "Train Epoch: 83 [38400/60000 (64%)]\tLoss: 1.476949\n",
            "Train Epoch: 83 [44800/60000 (75%)]\tLoss: 1.461208\n",
            "Train Epoch: 83 [51200/60000 (85%)]\tLoss: 1.461589\n",
            "Train Epoch: 83 [57600/60000 (96%)]\tLoss: 1.461563\n",
            "Train Epoch: 84 [0/60000 (0%)]\tLoss: 1.507352\n",
            "Train Epoch: 84 [6400/60000 (11%)]\tLoss: 1.476822\n",
            "Train Epoch: 84 [12800/60000 (21%)]\tLoss: 1.461733\n",
            "Train Epoch: 84 [19200/60000 (32%)]\tLoss: 1.462763\n",
            "Train Epoch: 84 [25600/60000 (43%)]\tLoss: 1.475126\n",
            "Train Epoch: 84 [32000/60000 (53%)]\tLoss: 1.476868\n",
            "Train Epoch: 84 [38400/60000 (64%)]\tLoss: 1.461160\n",
            "Train Epoch: 84 [44800/60000 (75%)]\tLoss: 1.462385\n",
            "Train Epoch: 84 [51200/60000 (85%)]\tLoss: 1.461166\n",
            "Train Epoch: 84 [57600/60000 (96%)]\tLoss: 1.476783\n",
            "Train Epoch: 85 [0/60000 (0%)]\tLoss: 1.461241\n",
            "Train Epoch: 85 [6400/60000 (11%)]\tLoss: 1.476826\n",
            "Train Epoch: 85 [12800/60000 (21%)]\tLoss: 1.476793\n",
            "Train Epoch: 85 [19200/60000 (32%)]\tLoss: 1.462214\n",
            "Train Epoch: 85 [25600/60000 (43%)]\tLoss: 1.476838\n",
            "Train Epoch: 85 [32000/60000 (53%)]\tLoss: 1.463398\n",
            "Train Epoch: 85 [38400/60000 (64%)]\tLoss: 1.461305\n",
            "Train Epoch: 85 [44800/60000 (75%)]\tLoss: 1.461215\n",
            "Train Epoch: 85 [51200/60000 (85%)]\tLoss: 1.461594\n",
            "Train Epoch: 85 [57600/60000 (96%)]\tLoss: 1.476607\n",
            "Train Epoch: 86 [0/60000 (0%)]\tLoss: 1.476789\n",
            "Train Epoch: 86 [6400/60000 (11%)]\tLoss: 1.461373\n",
            "Train Epoch: 86 [12800/60000 (21%)]\tLoss: 1.476782\n",
            "Train Epoch: 86 [19200/60000 (32%)]\tLoss: 1.490715\n",
            "Train Epoch: 86 [25600/60000 (43%)]\tLoss: 1.461438\n",
            "Train Epoch: 86 [32000/60000 (53%)]\tLoss: 1.461322\n",
            "Train Epoch: 86 [38400/60000 (64%)]\tLoss: 1.461169\n",
            "Train Epoch: 86 [44800/60000 (75%)]\tLoss: 1.461184\n",
            "Train Epoch: 86 [51200/60000 (85%)]\tLoss: 1.461294\n",
            "Train Epoch: 86 [57600/60000 (96%)]\tLoss: 1.461388\n",
            "Train Epoch: 87 [0/60000 (0%)]\tLoss: 1.476915\n",
            "Train Epoch: 87 [6400/60000 (11%)]\tLoss: 1.477058\n",
            "Train Epoch: 87 [12800/60000 (21%)]\tLoss: 1.476787\n",
            "Train Epoch: 87 [19200/60000 (32%)]\tLoss: 1.461645\n",
            "Train Epoch: 87 [25600/60000 (43%)]\tLoss: 1.477560\n",
            "Train Epoch: 87 [32000/60000 (53%)]\tLoss: 1.461191\n",
            "Train Epoch: 87 [38400/60000 (64%)]\tLoss: 1.478103\n",
            "Train Epoch: 87 [44800/60000 (75%)]\tLoss: 1.461407\n",
            "Train Epoch: 87 [51200/60000 (85%)]\tLoss: 1.461409\n",
            "Train Epoch: 87 [57600/60000 (96%)]\tLoss: 1.461341\n",
            "Train Epoch: 88 [0/60000 (0%)]\tLoss: 1.461860\n",
            "Train Epoch: 88 [6400/60000 (11%)]\tLoss: 1.461181\n",
            "Train Epoch: 88 [12800/60000 (21%)]\tLoss: 1.461228\n",
            "Train Epoch: 88 [19200/60000 (32%)]\tLoss: 1.461161\n",
            "Train Epoch: 88 [25600/60000 (43%)]\tLoss: 1.461237\n",
            "Train Epoch: 88 [32000/60000 (53%)]\tLoss: 1.461229\n",
            "Train Epoch: 88 [38400/60000 (64%)]\tLoss: 1.462466\n",
            "Train Epoch: 88 [44800/60000 (75%)]\tLoss: 1.461170\n",
            "Train Epoch: 88 [51200/60000 (85%)]\tLoss: 1.461206\n",
            "Train Epoch: 88 [57600/60000 (96%)]\tLoss: 1.461530\n",
            "Train Epoch: 89 [0/60000 (0%)]\tLoss: 1.476349\n",
            "Train Epoch: 89 [6400/60000 (11%)]\tLoss: 1.461197\n",
            "Train Epoch: 89 [12800/60000 (21%)]\tLoss: 1.461271\n",
            "Train Epoch: 89 [19200/60000 (32%)]\tLoss: 1.461777\n",
            "Train Epoch: 89 [25600/60000 (43%)]\tLoss: 1.476030\n",
            "Train Epoch: 89 [32000/60000 (53%)]\tLoss: 1.461380\n",
            "Train Epoch: 89 [38400/60000 (64%)]\tLoss: 1.476845\n",
            "Train Epoch: 89 [44800/60000 (75%)]\tLoss: 1.461206\n",
            "Train Epoch: 89 [51200/60000 (85%)]\tLoss: 1.461938\n",
            "Train Epoch: 89 [57600/60000 (96%)]\tLoss: 1.461164\n",
            "Train Epoch: 90 [0/60000 (0%)]\tLoss: 1.461522\n",
            "Train Epoch: 90 [6400/60000 (11%)]\tLoss: 1.461763\n",
            "Train Epoch: 90 [12800/60000 (21%)]\tLoss: 1.465076\n",
            "Train Epoch: 90 [19200/60000 (32%)]\tLoss: 1.461587\n",
            "Train Epoch: 90 [25600/60000 (43%)]\tLoss: 1.476265\n",
            "Train Epoch: 90 [32000/60000 (53%)]\tLoss: 1.461236\n",
            "Train Epoch: 90 [38400/60000 (64%)]\tLoss: 1.477249\n",
            "Train Epoch: 90 [44800/60000 (75%)]\tLoss: 1.461729\n",
            "Train Epoch: 90 [51200/60000 (85%)]\tLoss: 1.461575\n",
            "Train Epoch: 90 [57600/60000 (96%)]\tLoss: 1.476842\n",
            "Train Epoch: 91 [0/60000 (0%)]\tLoss: 1.461452\n",
            "Train Epoch: 91 [6400/60000 (11%)]\tLoss: 1.463676\n",
            "Train Epoch: 91 [12800/60000 (21%)]\tLoss: 1.461219\n",
            "Train Epoch: 91 [19200/60000 (32%)]\tLoss: 1.461879\n",
            "Train Epoch: 91 [25600/60000 (43%)]\tLoss: 1.462301\n",
            "Train Epoch: 91 [32000/60000 (53%)]\tLoss: 1.461182\n",
            "Train Epoch: 91 [38400/60000 (64%)]\tLoss: 1.462393\n",
            "Train Epoch: 91 [44800/60000 (75%)]\tLoss: 1.469781\n",
            "Train Epoch: 91 [51200/60000 (85%)]\tLoss: 1.461655\n",
            "Train Epoch: 91 [57600/60000 (96%)]\tLoss: 1.461246\n",
            "Train Epoch: 92 [0/60000 (0%)]\tLoss: 1.492547\n",
            "Train Epoch: 92 [6400/60000 (11%)]\tLoss: 1.476836\n",
            "Train Epoch: 92 [12800/60000 (21%)]\tLoss: 1.465893\n",
            "Train Epoch: 92 [19200/60000 (32%)]\tLoss: 1.462414\n",
            "Train Epoch: 92 [25600/60000 (43%)]\tLoss: 1.461872\n",
            "Train Epoch: 92 [32000/60000 (53%)]\tLoss: 1.462454\n",
            "Train Epoch: 92 [38400/60000 (64%)]\tLoss: 1.477578\n",
            "Train Epoch: 92 [44800/60000 (75%)]\tLoss: 1.461154\n",
            "Train Epoch: 92 [51200/60000 (85%)]\tLoss: 1.462031\n",
            "Train Epoch: 92 [57600/60000 (96%)]\tLoss: 1.462472\n",
            "Train Epoch: 93 [0/60000 (0%)]\tLoss: 1.461936\n",
            "Train Epoch: 93 [6400/60000 (11%)]\tLoss: 1.476881\n",
            "Train Epoch: 93 [12800/60000 (21%)]\tLoss: 1.461853\n",
            "Train Epoch: 93 [19200/60000 (32%)]\tLoss: 1.461504\n",
            "Train Epoch: 93 [25600/60000 (43%)]\tLoss: 1.461304\n",
            "Train Epoch: 93 [32000/60000 (53%)]\tLoss: 1.461589\n",
            "Train Epoch: 93 [38400/60000 (64%)]\tLoss: 1.461392\n",
            "Train Epoch: 93 [44800/60000 (75%)]\tLoss: 1.462309\n",
            "Train Epoch: 93 [51200/60000 (85%)]\tLoss: 1.461204\n",
            "Train Epoch: 93 [57600/60000 (96%)]\tLoss: 1.461327\n",
            "Train Epoch: 94 [0/60000 (0%)]\tLoss: 1.461152\n",
            "Train Epoch: 94 [6400/60000 (11%)]\tLoss: 1.477523\n",
            "Train Epoch: 94 [12800/60000 (21%)]\tLoss: 1.476790\n",
            "Train Epoch: 94 [19200/60000 (32%)]\tLoss: 1.461167\n",
            "Train Epoch: 94 [25600/60000 (43%)]\tLoss: 1.461362\n",
            "Train Epoch: 94 [32000/60000 (53%)]\tLoss: 1.492563\n",
            "Train Epoch: 94 [38400/60000 (64%)]\tLoss: 1.461666\n",
            "Train Epoch: 94 [44800/60000 (75%)]\tLoss: 1.461359\n",
            "Train Epoch: 94 [51200/60000 (85%)]\tLoss: 1.476706\n",
            "Train Epoch: 94 [57600/60000 (96%)]\tLoss: 1.461536\n",
            "Train Epoch: 95 [0/60000 (0%)]\tLoss: 1.461747\n",
            "Train Epoch: 95 [6400/60000 (11%)]\tLoss: 1.461976\n",
            "Train Epoch: 95 [12800/60000 (21%)]\tLoss: 1.461613\n",
            "Train Epoch: 95 [19200/60000 (32%)]\tLoss: 1.461276\n",
            "Train Epoch: 95 [25600/60000 (43%)]\tLoss: 1.461179\n",
            "Train Epoch: 95 [32000/60000 (53%)]\tLoss: 1.461221\n",
            "Train Epoch: 95 [38400/60000 (64%)]\tLoss: 1.461239\n",
            "Train Epoch: 95 [44800/60000 (75%)]\tLoss: 1.461166\n",
            "Train Epoch: 95 [51200/60000 (85%)]\tLoss: 1.461276\n",
            "Train Epoch: 95 [57600/60000 (96%)]\tLoss: 1.461378\n",
            "Train Epoch: 96 [0/60000 (0%)]\tLoss: 1.461607\n",
            "Train Epoch: 96 [6400/60000 (11%)]\tLoss: 1.462194\n",
            "Train Epoch: 96 [12800/60000 (21%)]\tLoss: 1.476807\n",
            "Train Epoch: 96 [19200/60000 (32%)]\tLoss: 1.462189\n",
            "Train Epoch: 96 [25600/60000 (43%)]\tLoss: 1.466041\n",
            "Train Epoch: 96 [32000/60000 (53%)]\tLoss: 1.461153\n",
            "Train Epoch: 96 [38400/60000 (64%)]\tLoss: 1.461376\n",
            "Train Epoch: 96 [44800/60000 (75%)]\tLoss: 1.461153\n",
            "Train Epoch: 96 [51200/60000 (85%)]\tLoss: 1.461535\n",
            "Train Epoch: 96 [57600/60000 (96%)]\tLoss: 1.461240\n",
            "Train Epoch: 97 [0/60000 (0%)]\tLoss: 1.461764\n",
            "Train Epoch: 97 [6400/60000 (11%)]\tLoss: 1.461355\n",
            "Train Epoch: 97 [12800/60000 (21%)]\tLoss: 1.461212\n",
            "Train Epoch: 97 [19200/60000 (32%)]\tLoss: 1.461155\n",
            "Train Epoch: 97 [25600/60000 (43%)]\tLoss: 1.461221\n",
            "Train Epoch: 97 [32000/60000 (53%)]\tLoss: 1.461604\n",
            "Train Epoch: 97 [38400/60000 (64%)]\tLoss: 1.461154\n",
            "Train Epoch: 97 [44800/60000 (75%)]\tLoss: 1.491498\n",
            "Train Epoch: 97 [51200/60000 (85%)]\tLoss: 1.461977\n",
            "Train Epoch: 97 [57600/60000 (96%)]\tLoss: 1.469593\n",
            "Train Epoch: 98 [0/60000 (0%)]\tLoss: 1.461185\n",
            "Train Epoch: 98 [6400/60000 (11%)]\tLoss: 1.461195\n",
            "Train Epoch: 98 [12800/60000 (21%)]\tLoss: 1.461218\n",
            "Train Epoch: 98 [19200/60000 (32%)]\tLoss: 1.476860\n",
            "Train Epoch: 98 [25600/60000 (43%)]\tLoss: 1.461182\n",
            "Train Epoch: 98 [32000/60000 (53%)]\tLoss: 1.476788\n",
            "Train Epoch: 98 [38400/60000 (64%)]\tLoss: 1.461152\n",
            "Train Epoch: 98 [44800/60000 (75%)]\tLoss: 1.476538\n",
            "Train Epoch: 98 [51200/60000 (85%)]\tLoss: 1.476822\n",
            "Train Epoch: 98 [57600/60000 (96%)]\tLoss: 1.461193\n",
            "Train Epoch: 99 [0/60000 (0%)]\tLoss: 1.476800\n",
            "Train Epoch: 99 [6400/60000 (11%)]\tLoss: 1.461219\n",
            "Train Epoch: 99 [12800/60000 (21%)]\tLoss: 1.461781\n",
            "Train Epoch: 99 [19200/60000 (32%)]\tLoss: 1.464097\n",
            "Train Epoch: 99 [25600/60000 (43%)]\tLoss: 1.461281\n",
            "Train Epoch: 99 [32000/60000 (53%)]\tLoss: 1.476931\n",
            "Train Epoch: 99 [38400/60000 (64%)]\tLoss: 1.461163\n",
            "Train Epoch: 99 [44800/60000 (75%)]\tLoss: 1.476797\n",
            "Train Epoch: 99 [51200/60000 (85%)]\tLoss: 1.461166\n",
            "Train Epoch: 99 [57600/60000 (96%)]\tLoss: 1.471740\n",
            "Train set: Average loss: 0.0229, Accuracy: 59800/60000 (100%)\n",
            "Test set: Average loss: 0.0231, Accuracy: 9904/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}